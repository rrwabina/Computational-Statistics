{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS321: Computational Statistics <br>\n",
    "\n",
    "##   Laboratory Exercise: Resampling Methods\n",
    "\n",
    "University of Science and Technology of Southern Philippines <br>\n",
    "\n",
    "## Student Name: <code>Student Name</code>\n",
    "\n",
    "\n",
    "Instructor: **Romen Samuel Wabina, MSc** <br>\n",
    "MSc Data Science and AI | Asian Institute of Technology <br>\n",
    "*ongoing* PhD Data Science (Healthcare and Clinical Informatics) \n",
    "\n",
    "\n",
    "### Instructions\n",
    "- Please submit this laboratory exercise as a **Jupyter Notebook file** <code>.ipynb</code> via email <code>romensamuelrodis.wab@student.mahidol.edu</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation\n",
    "KFold divides the samples into k groups (folds) of approximately equal sizes. Out of these k groups, k-1 folds are used for training and the remaning one is used for testing.\n",
    "This process is repeated k times using the code: <code> KFold(n_splits = 5, *, shuffle = False, random_state = None) </code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  [2 3 4 5] Test:  [0 1]\n",
      "Train:  [0 1 4 5] Test:  [2 3]\n",
      "Train:  [0 1 2 3] Test:  [4 5]\n"
     ]
    }
   ],
   "source": [
    "X = [\"a\",'b','c','d','e','f']\n",
    "\n",
    "kf = KFold(n_splits = 3,shuffle = False, random_state = None)\n",
    "for train, test in kf.split(X):\n",
    "    print('Train: ', train, 'Test: ', test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-Fold \n",
    "\n",
    "This technique is a variation of K-Fold, and it divides the data into k-stratified folds. This way it preserves the percentage of samples of each class present in the data. It generates test sets such that all sets contain the same distribution of classes, or as close as possible\n",
    "\n",
    "<code>sklearn.model_selection.StratifiedKFold(n_splits = 5, *, shuffle = False, random_state = None)</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  [1 3 4 5] Test:  [0 2]\n",
      "Train:  [0 2 3 5] Test:  [1 4]\n",
      "Train:  [0 1 2 4] Test:  [3 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array([[1,2], [3,4], [5,6], [7,8], [9,10], [11,12]])\n",
    "y = np.array([0, 0, 1, 0, 1, 1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3, random_state = None, shuffle = False)\n",
    "\n",
    "for train_index,test_index in skf.split(X,y):\n",
    "    print('Train: ', train_index, 'Test: ', test_index)\n",
    "    X_train,X_test = X[train_index], X[test_index]\n",
    "    y_train,y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave One Out Cross Validation\n",
    "\n",
    "This is a simple technique in which training data inlcudes all observations in the data except one observation which will be used to test. For $n$ samples, we have $n$ different training sets. Although this model is trained on almost all of the data, the number of iterations and n different training sets, makes it computationally very expensive. Almost all of the data ($n-1$ of the $n$ samples) is used to build each model, all of the models are identical to each other and this results in high variance compared KFold.\n",
    "\n",
    "<code>sklearn.model_selection import LeaveOneOut()</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9] [0]\n",
      "[0 2 3 4 5 6 7 8 9] [1]\n",
      "[0 1 3 4 5 6 7 8 9] [2]\n",
      "[0 1 2 4 5 6 7 8 9] [3]\n",
      "[0 1 2 3 5 6 7 8 9] [4]\n",
      "[0 1 2 3 4 6 7 8 9] [5]\n",
      "[0 1 2 3 4 5 7 8 9] [6]\n",
      "[0 1 2 3 4 5 6 8 9] [7]\n",
      "[0 1 2 3 4 5 6 7 9] [8]\n",
      "[0 1 2 3 4 5 6 7 8] [9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "l = LeaveOneOut()\n",
    "\n",
    "for train, test in l.split(X):\n",
    "    print(\"%s %s\"% (train,test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <code>Question 1 (10 points): Explain the advantages and disadvantages of the cross-validation methods above. You can provide examples and scenarios to strengthen your arguments in this query and achieve full marks</code>. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Cross Validation\n",
    "\n",
    "The GridSearch Cross Validation (GridSearchCV) is the widely-used CV method in machine learning. GridSearchCV (Grid Search Cross Validation) is a technique used for hyperparameter tuning in machine learning models. It is a method of systematically searching for the best combination of hyperparameters to optimize the model's performance.\n",
    "\n",
    "In GridSearchCV, a range of hyperparameter values is defined for each hyperparameter of the model, and the algorithm exhaustively searches all possible combinations of hyperparameters using cross-validation to evaluate each combination's performance. The model is trained on various subsets of the data and evaluated on the remaining data. The process is repeated for each combination of hyperparameters.\n",
    "\n",
    "GridSearchCV is widely used in machine learning to find the best combination of hyperparameters for a given model. It is an essential tool for fine-tuning models to achieve better performance. The resulting hyperparameters can then be used to train the final model with the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from time import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colon = pd.read_csv('data/colon.csv')\n",
    "XX = df_colon.drop('Class', axis = 1)\n",
    "yy = df_colon['Class']\n",
    "\n",
    "X, y = XX.to_numpy(), yy.to_numpy()\n",
    "y = y.flatten()\n",
    "\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "random.seed(413)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "random.seed(413)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains a machine learning model using k-Nearest Neighbors (KNN) algorithm and performs hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "The <code>main()</code> function takes in training and testing data, the KNN model to use, and a dictionary of hyperparameters to be tuned. It also takes in a boolean variable display_train to decide whether to display the performance metrics of the training set.\n",
    "\n",
    "The <code>GridSearchCV()</code> method performs an exhaustive search over a specified hyperparameter space to find the best combination of hyperparameters that optimize the given scoring metric. In this case, the metric used is 'recall', which is a measure of the proportion of true positives that were correctly classified. The cv parameter specifies the number of cross-validation folds to be used, and refit parameter tells the <code>GridSearchCV()</code> to use the 'recall' scoring metric to select the best hyperparameters.\n",
    "\n",
    "After fitting the grid search to the training data, the best hyperparameters are printed using the best_params_ and best_score_ attributes of the <code>GridSearchCV()</code> object.\n",
    "\n",
    "Finally, the predictions and yhat variables are used to make predictions on the training and testing data, respectively. The display_results() function prints the confusion matrix and classification report for both the training and testing sets, which include metrics such as precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def display_results(y, prediction, set = 'Training Set'):\n",
    "     print(f'======================= {set} =======================')\n",
    "     print(confusion_matrix(y, prediction))\n",
    "     print(classification_report(y, prediction, target_names = ['No Cancer', 'Cancer']))\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test, model, param_grid = {'n_neighbors': [1, 2]}, display_train = True):\n",
    "     start = time()\n",
    "     \n",
    "     cv = StratifiedShuffleSplit(n_splits = 10, random_state = 42)\n",
    "     \n",
    "     random.seed(413)\n",
    "\n",
    "     # We used the accuracy as a refit to determine the best samples with the best accuracy\n",
    "     grid = GridSearchCV(model, param_grid = param_grid, cv = cv, refit = 'accuracy')\n",
    "     grid.fit(X_train, y_train) \n",
    "\n",
    "     print(f'The best parameters are {grid.best_params_} with a score of {grid.best_score_:.2f}')\n",
    "     \n",
    "     predictions = grid.predict(X_train)\n",
    "     yhat = grid.predict(X_test)\n",
    "     print(f\"Fit and predict time: {np.round(time() - start, 4)} seconds\")\n",
    "     if display_train:\n",
    "          display_results(y_train, predictions, set = 'Training Set')\n",
    "     display_results(y_test, yhat, set = 'Testing Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 1} with a score of 0.89\n",
      "Fit and predict time: 4.0486 seconds\n",
      "======================= Testing Set =======================\n",
      "[[7 1]\n",
      " [0 8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Cancer       1.00      0.88      0.93         8\n",
      "      Cancer       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.94        16\n",
      "   macro avg       0.94      0.94      0.94        16\n",
      "weighted avg       0.94      0.94      0.94        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_distance = KNeighborsClassifier(weights = 'distance')\n",
    "main(X_train, X_test, y_train, y_test, model_distance, display_train = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <code>Question 2 (10 points): Modify the code above and train the model by using KNN at neighbors [3,12] using recall as its refit strategy. Determine which number of neighbors got the best parameters with the highest recall</code>. \n",
    "\n",
    "#### <code>Question 3 (5 points): Explain the benefits that make GridSearchCV a popular choice among data scientists and machine learning practitioners</code>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
