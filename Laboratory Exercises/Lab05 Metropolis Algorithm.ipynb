{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS321: Computational Statistics <br>\n",
    "\n",
    "##   Lecture: Metropolis-Hastings Algorithm\n",
    "\n",
    "University of Science and Technology of Southern Philippines <br>\n",
    "\n",
    "## Student Name: <code>Romen Samuel Wabina</code>\n",
    "\n",
    "Instructor: **Romen Samuel Wabina, MSc** <br>\n",
    "MSc Data Science and AI | Asian Institute of Technology <br>\n",
    "*ongoing* PhD Data Science (Healthcare and Clinical Informatics) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCMC algorithms are algorithms that samples from complex probability distributions. They are commonly used when it is difficult or impossible to directly sample from a target probability distribution. \n",
    "\n",
    "A Markov Chain is is a chain of discrete events where the probability of the next event is conditioned only upon the current event. In this system of discrete choices, there exists a transition matrix, which quantifies the probability of transitioning from any given state to any given state. A Monte Carlo method is really just a fancy name for a simulation/experiment that relies on usage of random numbers.\n",
    "\n",
    "## Introduction: Metropolis-Hastings Algorithm\n",
    "\n",
    "The Metropolis-Hastings algorithm is a Markov Chain Monte Carlo (MCMC) method used to generate samples from a probability distribution when direct sampling is difficult or infeasible. It is named after the scientists Nicholas Metropolis and W. Keith Hastings, who introduced the algorithm in the 1950s. The key idea of the algorithm is that it constructs a Markov chain in which the samples are generated sequentially, and the distribution of the samples converges to the desired target distribution over time. The acceptance probability ensures that the chain explores the sample space in a way that converges to the correct distribution.\n",
    "\n",
    "\n",
    "Let's take a look at this Bayes' formula:\n",
    "$$ P(\\theta|x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Sampling\n",
    "\n",
    "Let's take a look at this Bayes' formula:\n",
    "$$ P(\\theta|x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled States (First Sample): [0, 0, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def monte_carlo_markov_chain(transition_matrix, initial_state, num_samples, num_steps):\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        current_state = initial_state\n",
    "        states = [current_state]\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            transition_probabilities = transition_matrix[current_state]\n",
    "\n",
    "            # Direct Sampling\n",
    "            next_state = np.random.choice(len(transition_probabilities), p = transition_probabilities)\n",
    "            current_state = next_state\n",
    "            states.append(current_state)\n",
    "\n",
    "            # Stop sampling if a terminal state (e.g., Death) is reached\n",
    "            if current_state == len(transition_matrix) - 1:\n",
    "                break\n",
    "\n",
    "        samples.append(states)\n",
    "    return samples\n",
    "\n",
    "# Transition matrix representing the Markov Chain\n",
    "transition_matrix = np.array([\n",
    "    [0.5, 0.4, 0.1, 0.0, 0.0],\n",
    "    [0.0, 0.6, 0.3, 0.1, 0.0],\n",
    "    [0.0, 0.0, 0.7, 0.2, 0.1],\n",
    "    [0.0, 0.0, 0.0, 0.8, 0.2],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "initial_state = 0   # Starting from Stage 3\n",
    "num_samples = 10    # Number of Monte Carlo samples\n",
    "num_steps = 10      # Number of steps to sample\n",
    "\n",
    "# Perform Monte Carlo sampling with the Markov Chain\n",
    "sampled_states = monte_carlo_markov_chain(transition_matrix, initial_state, num_samples, num_steps)\n",
    "\n",
    "# Print the sampled states for the first sample\n",
    "print(\"Sampled States (First Sample):\", sampled_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 0 with CKD Progression: \t {0, 2, 3, 4}\n",
      "Patient 1 with CKD Progression: \t {0, 1, 2}\n",
      "Patient 2 with CKD Progression: \t {0, 1, 2}\n",
      "Patient 3 with CKD Progression: \t {0, 1, 2, 3, 4}\n",
      "Patient 4 with CKD Progression: \t {0, 1, 2, 3}\n",
      "Patient 5 with CKD Progression: \t {0, 1, 2, 3}\n",
      "Patient 6 with CKD Progression: \t {0, 1, 2, 4}\n",
      "Patient 7 with CKD Progression: \t {0, 2, 4}\n",
      "Patient 8 with CKD Progression: \t {0, 1, 2, 4}\n",
      "Patient 9 with CKD Progression: \t {0, 1, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "for idx, state in enumerate(sampled_states):\n",
    "    print(f'Patient {idx} with CKD Progression: \\t {set(state)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a Markov Chain model for chronic kidney disease progression, direct sampling is not inherently difficult because the Markov Chain allows for the direct generation of samples. The transition probabilities between different stages of the disease and other health states determine the probabilities of transitioning from one state to another. However, direct sampling have disadvantages:\n",
    "\n",
    "1. Direct sampling can become challenging in high-dimensional spaces due to the \"curse of dimensionality.\" Obtaining representative samples becomes computationally expensive or infeasible as the number of dimensions increases.\n",
    "2. In some cases, the distribution of CKD stages and associated risk factors may be analytically or computationally intractable, making direct sampling challenging. If the distributions cannot be easily parameterized or sampled from, alternative methods like importance sampling or MCMC techniques may be more suitable.\n",
    "3. Direct sampling can be computationally expensive, especially if the CKD model involves complex calculations, simulations, or computationally intensive algorithms. Sampling directly from a large dataset or running extensive simulations may not be feasible in real-time applications or when dealing with a massive amount of data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling\n",
    "\n",
    "Importance sampling is a Markov Chain Monte Carlo method for evaluating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. Importance sampling is a variance reduction technique that can be used in the Monte Carlo method. The idea behind importance sampling is that certain values of the input random variables in a simulation have more impact on the parameter being estimated than others. If these \"important\" values are emphasized by sampling more frequently, then the estimator variance can be reduced. Hence, the basic methodology in importance sampling is to choose a distribution which \"encourages\" the important values.\n",
    "\n",
    "\n",
    "With importance sampling, we try to reduce the variance of our Monte-Carlo integral estimation by choosing a better distribution from which to simulate our random variables. It involves multiplying the integrand by 1 (usually dressed up in a “tricky fashion”) to yield an expectation of a quantity that varies less than the original integrand over the region of integration. Concretely,\n",
    "\n",
    "$$\\mathbb{E}_{p(x)} \\big[\\ f(x) \\big] = \\int f(x)\\ p(x)\\ dx = \\int f(x)\\ p(x)\\ \\frac{q(x)}{q(x)}\\ dx = \\int \\frac{p(x)}{q(x)}\\cdot f(x)\\ q(x)\\ dx = \\mathbb{E}_{q(x)}  \\big[\\ f(x)\\cdot \\frac{p(x)}{q(x)} \\big]$$\n",
    "\n",
    "Thus, the MC estimation of the expectation becomes:\n",
    "\n",
    "$$\\mathbb{E}_{q(x)}  \\big[\\ f(x)\\cdot \\frac{p(x)}{q(x)} \\big] \\approx \\frac{1}{N} \\sum_{n=1}^{N} w_n \\cdot f(x_n)$$\n",
    "\n",
    "where $w_n = \\dfrac{p(x_n)}{q(x_n)}$\n",
    "\n",
    "\n",
    "In importance sampling, the proposal distribution is a probability distribution used to generate samples during the sampling process. It serves as an approximation to the target distribution from which you want to estimate properties or calculate expectations. The proposal distribution should ideally have a similar shape or be closely related to the target distribution to ensure accurate importance sampling estimates. The primary purpose of the proposal distribution is to efficiently explore the space of possible samples and assign appropriate weights to each sample. The importance weights, which are the ratios of the target distribution's density to the proposal distribution's density, are used to adjust the contribution of each sample to the final estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sampling_markov_chain(transition_matrix, proposal_matrix, initial_state, num_samples, num_steps):\n",
    "    samples = []\n",
    "    weights = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        current_state = initial_state\n",
    "        states = [current_state]\n",
    "        weight = 1.0\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            transition_probabilities = transition_matrix[current_state]\n",
    "            proposal_probabilities   = proposal_matrix[current_state]\n",
    "            \n",
    "            # Sample the next state using the proposal distribution\n",
    "            next_state = np.random.choice(len(proposal_probabilities), p = proposal_probabilities)\n",
    "            \n",
    "            # Calculate the weight using the importance sampling ratio\n",
    "            weight *= transition_probabilities[next_state] / proposal_probabilities[next_state]\n",
    "            \n",
    "            current_state = next_state\n",
    "            states.append(current_state)\n",
    "\n",
    "            # Stop sampling if a terminal state (e.g., Death) is reached\n",
    "            if current_state == len(transition_matrix) - 1:\n",
    "                break\n",
    "        \n",
    "        samples.append(states)\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Normalize the weights\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    return samples, weights\n",
    "\n",
    "# Transition matrix representing the Markov Chain\n",
    "transition_matrix = np.array([\n",
    "    [0.5, 0.4, 0.1, 0.0, 0.0],\n",
    "    [0.0, 0.6, 0.3, 0.1, 0.0],\n",
    "    [0.0, 0.0, 0.7, 0.2, 0.1],\n",
    "    [0.0, 0.0, 0.0, 0.8, 0.2],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Proposal matrix representing the proposal distribution (can be different from the transition matrix)\n",
    "# The proposal distribution is a probability distribution used to generate samples during the sampling \n",
    "# process. It serves as an approximation to the target distribution from which you want to estimate \n",
    "# properties or calculate expectations. The proposal distribution should ideally have a similar shape \n",
    "# or be closely related to the target distribution to ensure accurate importance sampling estimates.\n",
    "proposal_matrix = np.array([\n",
    "    [0.3, 0.3, 0.2, 0.1, 0.1],\n",
    "    [0.1, 0.5, 0.2, 0.1, 0.1],\n",
    "    [0.0, 0.1, 0.6, 0.2, 0.1],\n",
    "    [0.0, 0.0, 0.1, 0.6, 0.3],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "initial_state = 0  # Starting from Stage 3\n",
    "num_samples = 10   # Number of importance samples\n",
    "num_steps = 10     # Number of steps to sample\n",
    "\n",
    "# Perform importance sampling with the Markov Chain\n",
    "sampled_states, importance_weights = importance_sampling_markov_chain(\n",
    "    transition_matrix, proposal_matrix, initial_state, num_samples, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 0 with CKD Progression: \t {0, 2, 3, 4}\n",
      "Patient 1 with CKD Progression: \t {0, 2, 4}\n",
      "Patient 2 with CKD Progression: \t {0, 1, 2, 4}\n",
      "Patient 3 with CKD Progression: \t {0, 1, 2, 3}\n",
      "Patient 4 with CKD Progression: \t {0, 2, 3, 4}\n",
      "Patient 5 with CKD Progression: \t {0, 1, 2, 4}\n",
      "Patient 6 with CKD Progression: \t {0, 2, 3, 4}\n",
      "Patient 7 with CKD Progression: \t {0, 1, 3, 4}\n",
      "Patient 8 with CKD Progression: \t {0, 3, 4}\n",
      "Patient 9 with CKD Progression: \t {0, 1, 3}\n"
     ]
    }
   ],
   "source": [
    "for idx, state in enumerate(sampled_states):\n",
    "    print(f'Patient {idx} with CKD Progression: \\t {set(state)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance Sampling:\n",
      "Estimated Mean: 4.815660868077247\n",
      "\n",
      "Metropolis-Hastings:\n",
      "Estimated Mean: 4.984573440505409\n",
      "Acceptance Ratio: 0.614909090909091\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def target_distribution(x):\n",
    "    \"\"\"Target distribution: Gaussian with mean 5 and standard deviation 2.\"\"\"\n",
    "    return np.exp(-(x - 5) ** 2 / 8) / np.sqrt(8 * np.pi)\n",
    "\n",
    "def proposal_distribution():\n",
    "    \"\"\"Proposal distribution: Uniform distribution between 0 and 10.\"\"\"\n",
    "    return np.random.uniform(0, 10)\n",
    "\n",
    "def importance_sampling(n_samples):\n",
    "    \"\"\"Implementation of Importance Sampling.\"\"\"\n",
    "    samples = np.zeros(n_samples)\n",
    "    weights = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        sample = proposal_distribution()\n",
    "        weight = target_distribution(sample) / proposal_distribution()\n",
    "        samples[i] = sample\n",
    "        weights[i] = weight\n",
    "\n",
    "    weights /= np.sum(weights)  # Normalize the weights\n",
    "    estimated_mean = np.sum(samples * weights)\n",
    "\n",
    "    return estimated_mean\n",
    "\n",
    "def metropolis_hastings(n_samples, burn_in=1000):\n",
    "    \"\"\"Implementation of Metropolis-Hastings.\"\"\"\n",
    "    samples = np.zeros(n_samples + burn_in)\n",
    "    accepted = 0\n",
    "    current_sample = proposal_distribution()\n",
    "\n",
    "    for i in range(n_samples + burn_in):\n",
    "        proposal = proposal_distribution()\n",
    "        acceptance_prob = min(1, target_distribution(proposal) / target_distribution(current_sample))\n",
    "\n",
    "        if np.random.rand() < acceptance_prob:\n",
    "            current_sample = proposal\n",
    "            accepted += 1\n",
    "\n",
    "        samples[i] = current_sample\n",
    "\n",
    "    samples = samples[burn_in:]\n",
    "    estimated_mean = np.mean(samples)\n",
    "\n",
    "    return estimated_mean, accepted / (n_samples + burn_in)\n",
    "\n",
    "# Testing the implementations\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "# Importance Sampling\n",
    "estimated_mean_importance = importance_sampling(n_samples)\n",
    "print(\"Importance Sampling:\")\n",
    "print(\"Estimated Mean:\", estimated_mean_importance)\n",
    "\n",
    "# Metropolis-Hastings\n",
    "estimated_mean_mh, acceptance_ratio = metropolis_hastings(n_samples)\n",
    "print(\"\\nMetropolis-Hastings:\")\n",
    "print(\"Estimated Mean:\", estimated_mean_mh)\n",
    "print(\"Acceptance Ratio:\", acceptance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled States (First Sample): [0, 4]\n",
      "Importance Weights (First Sample): 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def importance_sampling_markov_chain(transition_matrix, pdf_func, initial_state, num_samples, num_steps):\n",
    "    samples = []\n",
    "    weights = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        current_state = initial_state\n",
    "        states = [current_state]\n",
    "        weight = 1.0\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            transition_probabilities = transition_matrix[current_state]\n",
    "            \n",
    "            # Generate a sample from the proposal distribution using the given PDF\n",
    "            proposal_probabilities = pdf_func(current_state)\n",
    "            next_state = np.random.choice(len(proposal_probabilities), p=proposal_probabilities)\n",
    "            \n",
    "            # Calculate the weight using the importance sampling ratio\n",
    "            weight *= transition_probabilities[next_state] / proposal_probabilities[next_state]\n",
    "            \n",
    "            current_state = next_state\n",
    "            states.append(current_state)\n",
    "\n",
    "            # Stop sampling if a terminal state (e.g., Death) is reached\n",
    "            if current_state == len(transition_matrix) - 1:\n",
    "                break\n",
    "        \n",
    "        samples.append(states)\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Normalize the weights\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    return samples, weights\n",
    "\n",
    "# Transition matrix representing the Markov Chain\n",
    "transition_matrix = np.array([\n",
    "    [0.5, 0.4, 0.1, 0.0, 0.0],\n",
    "    [0.0, 0.6, 0.3, 0.1, 0.0],\n",
    "    [0.0, 0.0, 0.7, 0.2, 0.1],\n",
    "    [0.0, 0.0, 0.0, 0.8, 0.2],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "initial_state = 0  # Starting from Stage 3\n",
    "num_samples = 1000  # Number of importance samples\n",
    "num_steps = 10  # Number of steps to sample\n",
    "\n",
    "# Define the proposal probability density function (PDF)\n",
    "def proposal_pdf(state):\n",
    "    # Example: Uniform distribution as the proposal PDF\n",
    "    num_states = len(transition_matrix)\n",
    "    probabilities = np.full(num_states, 1/num_states)\n",
    "    return probabilities\n",
    "\n",
    "# Perform importance sampling with the Markov Chain\n",
    "sampled_states, importance_weights = importance_sampling_markov_chain(\n",
    "    transition_matrix, proposal_pdf, initial_state, num_samples, num_steps\n",
    ")\n",
    "\n",
    "# Print the sampled states and their corresponding importance weights for the first sample\n",
    "print(\"Sampled States (First Sample):\", sampled_states[0])\n",
    "print(\"Importance Weights (First Sample):\", importance_weights[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
